# Vertex AI Customer Churn Prediction

ğŸš€ An end-to-end **ML pipeline on Google Cloud Vertex AI**, built to learn the basics of **data engineering + data science workflows in GCP**.  
This project trains a simple model to predict whether a customer/person falls into a **"high income" category (>100K)** (proxy for churn/no churn).  

## ğŸ“Œ Objectives
- Learn the basics of **Google Cloud Vertex AI** (pipelines, model training, deployment).  
- Practice **Python + ML integration with GCP services**.  
- Showcase an **end-to-end project** for interviews and portfolio.  
- Use **only free resources** within Google Cloudâ€™s free tier.  

---

## ğŸ—‚ï¸ Project Structure
```bash
vertex-ai-churn-prediction/
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ scripts/
â”œâ”€â”€ data/                  # runtime
â”œâ”€â”€ models/                # runtime
â”œâ”€â”€ pipeline/
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ register_model_vertex_ai.py
â””â”€â”€ deployment/
    â”œâ”€â”€ deploy_cloud_run.sh
    â””â”€â”€ app/
        â””â”€â”€ main.py
```
---

## ğŸ› ï¸ Tech Stack
- **Google Cloud Platform**  
  - Vertex AI  
  - BigQuery (public datasets)  
  - Cloud Storage  
- **Python**  
  - pandas, scikit-learn, joblib  
  - google-cloud-sdk  

---

## ğŸš€ Steps to Run

### 1ï¸âƒ£ Setup GCP
1. Create a new GCP project (free tier).  
2. Enable APIs: Vertex AI, BigQuery, Cloud Storage.  
3. Authenticate:
   ```bash
   gcloud init
   gcloud auth login



Perfect ğŸ‰ â€” having a clear **README.md** will make your GitHub repo shine in interviews. Hereâ€™s a draft you can copy into your repoâ€™s `README.md` (or merge into your existing one). It explains the **project goal**, **steps**, and how to reproduce everything on GCP.

---

# ğŸ“Š Vertex AI Churn Prediction (Census Income Dataset)

This project is an **end-to-end ML pipeline on Google Cloud** using **free-tier resources**.
It shows how to:

* Prepare data from BigQuery public datasets
* Train a scikit-learn pipeline locally
* Package the model in a FastAPI service
* Deploy it as a **serverless API on Cloud Run**

ğŸ‘‰ Great for learning **Data Scientist / Data Engineer workflows** in GCP.

---

## ğŸš€ Tech Stack

* **Python 3.11**
* **scikit-learn** (Logistic Regression, preprocessing pipeline)
* **FastAPI** + **Uvicorn** (serving)
* **Docker** (containerization, built via Cloud Build)
* **Google Cloud**: BigQuery, Cloud Storage (optional), Artifact Registry, Cloud Run

---

## ğŸ“‚ Project Structure

```
vertex-ai-churn-prediction/
â”œâ”€â”€ Dockerfile                 # Container config (root level for Cloud Build)
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pipeline/                  # Data prep + training scripts
â”‚   â”œâ”€â”€ data_prep.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ register_model_vertex_ai.py  # (optional)
â”œâ”€â”€ models/                    # Saved model + metrics (generated at runtime)
â”œâ”€â”€ data/                      # Train/test CSVs (generated at runtime)
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ app/                   # FastAPI serving app
â”‚   â”‚   â””â”€â”€ main.py
â”‚   â””â”€â”€ deploy_cloud_run.sh    # Deployment script
â””â”€â”€ README.md
```

---

## ğŸ“ Setup Instructions

### 1. Clone repo

```bash
git clone https://github.com/YOUR_USERNAME/vertex-ai-churn-prediction.git
cd vertex-ai-churn-prediction
```

### 2. Authenticate GCP

```bash
gcloud auth login
gcloud auth application-default login
gcloud config set project YOUR_PROJECT_ID
```

### 3. Enable required APIs

```bash
./scripts/enable_apis.sh
```

### 4. Prepare data

```bash
python pipeline/data_prep.py --project_id YOUR_PROJECT_ID
```

This pulls the **Census Income dataset** from BigQuery and creates:

* `data/train.csv`
* `data/test.csv`

### 5. Train model

```bash
python pipeline/train.py
```

Outputs:

* `models/model.joblib` â†’ trained pipeline
* `models/metrics.json` â†’ evaluation metrics
* `models/schema.json` â†’ feature schema

### 6. Deploy model to Cloud Run

```bash
./deployment/deploy_cloud_run.sh
```

This will:

* Build & push Docker image to **Artifact Registry**
* Deploy the container to **Cloud Run**
* Print a public URL for the service

---

## ğŸ§ª Test the API

```bash
curl -X POST https://YOUR_CLOUD_RUN_URL/predict \
  -H "Content-Type: application/json" \
  -d '{"records":[{"features":{"age":39,"workclass":"Private","education":"Bachelors","marital_status":"Never-married","occupation":"Adm-clerical","relationship":"Not-in-family","race":"White","sex":"Male","capital_gain":2174,"capital_loss":0,"hours_per_week":40,"native_country":"United-States"}}]}'
```

Example response:

```json
{"predictions": [0], "probabilities": [0.23]}
```

---

## ğŸ¯ Why this project?

* Shows **full lifecycle**: data â†’ training â†’ deployment
* Uses **only free-tier GCP services**
* Mirrors **real-world MLops workflows**:

  * Reproducible pipelines with sklearn
  * Containerized deployment
  * Serverless inference API

---

## ğŸ”® Possible Extensions

* Swap Logistic Regression for XGBoost or TensorFlow
* Register the model in **Vertex AI Model Registry**
* Automate retraining with **Vertex AI Pipelines**
* Add CI/CD with **Cloud Build triggers**
